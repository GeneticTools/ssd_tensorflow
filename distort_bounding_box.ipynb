{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "pRF-gtU1kac2",
    "outputId": "1349656e-3441-4f45-d066-5cc5e9272d8c"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "from tqdm import tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from ssd.common.viz_utils import draw_boxes_cv2, imshow, imshow_multiple\n",
    "from ssd.common.box_utils import rescale_boxes\n",
    "\n",
    "logger = tf.get_logger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "logger.info('version: {}'.format(tf.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_example(example_proto, _input_height=448, _input_width=448):\n",
    "    feature_description = {\n",
    "        'image': tf.io.FixedLenFeature([], tf.string),\n",
    "        'xmins': tf.io.VarLenFeature(tf.float32),\n",
    "        'ymins': tf.io.VarLenFeature(tf.float32),\n",
    "        'xmaxs': tf.io.VarLenFeature(tf.float32),\n",
    "        'ymaxs': tf.io.VarLenFeature(tf.float32),\n",
    "        'classes': tf.io.VarLenFeature(tf.int64),\n",
    "    }\n",
    "\n",
    "    parsed_example = tf.io.parse_single_example(example_proto,\n",
    "                                                feature_description)\n",
    "    classes = tf.sparse.to_dense(parsed_example['classes'])\n",
    "\n",
    "    image = tf.io.decode_image(parsed_example['image'], channels=3)\n",
    "    image = tf.cast(image, dtype=tf.float32)\n",
    "    image.set_shape([None, None, 3])\n",
    "    original_dims = tf.shape(image)\n",
    "    image = tf.image.resize(image,\n",
    "                            size=[_input_height, _input_width])\n",
    "\n",
    "    boxes = tf.stack([\n",
    "        tf.sparse.to_dense(parsed_example['xmins']),\n",
    "        tf.sparse.to_dense(parsed_example['ymins']),\n",
    "        tf.sparse.to_dense(parsed_example['xmaxs']),\n",
    "        tf.sparse.to_dense(parsed_example['ymaxs']),\n",
    "    ], axis=-1)\n",
    "    boxes = rescale_boxes(boxes,\n",
    "                          [original_dims[0], original_dims[1]],\n",
    "                          [_input_height, _input_width])\n",
    "    return image, boxes, classes\n",
    "\n",
    "def relative_to_absolute(boxes, image_dims):\n",
    "    boxes = tf.cast(boxes, dtype=tf.float32)\n",
    "    image_dims = tf.cast(image_dims, dtype=tf.float32)\n",
    "    return tf.stack([\n",
    "        boxes[..., 0] * image_dims[1],\n",
    "        boxes[..., 1] * image_dims[0],\n",
    "        boxes[..., 2] * image_dims[1],\n",
    "        boxes[..., 3] * image_dims[0]\n",
    "    ], axis=-1)\n",
    "\n",
    "def absolute_to_relative(boxes, image_dims):\n",
    "    boxes = tf.cast(boxes, dtype=tf.float32)\n",
    "    image_dims = tf.cast(image_dims, dtype=tf.float32)\n",
    "    return tf.stack([\n",
    "        boxes[..., 0] / image_dims[1],\n",
    "        boxes[..., 1] / image_dims[0],\n",
    "        boxes[..., 2] / image_dims[1],\n",
    "        boxes[..., 3] / image_dims[0]\n",
    "    ], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_tfrecords = tf.data.Dataset.list_files('ssd/data/dataset_files/shapes_dataset/train*')\n",
    "dataset = _tfrecords.interleave(\n",
    "    tf.data.TFRecordDataset,\n",
    "    cycle_length=8,\n",
    "    block_length=32,\n",
    "    num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "dataset = dataset.shuffle(512)\n",
    "dataset = dataset.map(_parse_example,\n",
    "                      num_parallel_calls=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swap_xy(boxes):\n",
    "    boxes = tf.cast(boxes, dtype=tf.float32)\n",
    "    return tf.stack([\n",
    "        boxes[:, 1],\n",
    "        boxes[:, 0],\n",
    "        boxes[:, 3],\n",
    "        boxes[:, 2],\n",
    "    ], axis=-1)\n",
    "\n",
    "def relative_to_absolute(boxes, image_dims):\n",
    "    boxes = tf.cast(boxes, dtype=tf.float32)\n",
    "    image_dims = tf.cast(image_dims, dtype=tf.float32)\n",
    "    return tf.stack([\n",
    "        boxes[..., 0] * image_dims[1],\n",
    "        boxes[..., 1] * image_dims[0],\n",
    "        boxes[..., 2] * image_dims[1],\n",
    "        boxes[..., 3] * image_dims[0]\n",
    "    ], axis=-1)\n",
    "\n",
    "def absolute_to_relative(boxes, image_dims):\n",
    "    boxes = tf.cast(boxes, dtype=tf.float32)\n",
    "    image_dims = tf.cast(image_dims, dtype=tf.float32)\n",
    "    return tf.stack([\n",
    "        boxes[..., 0] / image_dims[1],\n",
    "        boxes[..., 1] / image_dims[0],\n",
    "        boxes[..., 2] / image_dims[1],\n",
    "        boxes[..., 3] / image_dims[0]\n",
    "    ], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_and_adjust_labels(crop_box, boxes, classes):\n",
    "    boxes = tf.cast(boxes, dtype=tf.float32)\n",
    "    crop_box = tf.cast(crop_box, dtype=tf.float32)\n",
    "    \n",
    "    offsets = tf.concat([\n",
    "        crop_box[:, :2] - boxes[:, :2], \n",
    "        boxes[:, 2:] - crop_box[:, 2:],\n",
    "    ], axis=-1)\n",
    "    \n",
    "    crop_box_width = crop_box[:, 2] - crop_box[:, 0]\n",
    "    crop_box_height = crop_box[:, 3] - crop_box[:, 1]\n",
    "    \n",
    "    adjusted_boxes = tf.stack([\n",
    "        tf.clip_by_value(boxes[:, 0] - crop_box[:, 0], 0, crop_box_width),\n",
    "        tf.clip_by_value(boxes[:, 1] - crop_box[:, 1], 0, crop_box_height),\n",
    "        tf.clip_by_value(boxes[:, 2] - crop_box[:, 0], 0, crop_box_width),\n",
    "        tf.clip_by_value(boxes[:, 3] - crop_box[:, 1], 0, crop_box_height)\n",
    "    ], axis=-1)\n",
    "\n",
    "    idx = tf.where(tf.logical_not(tf.reduce_all(offsets >= 0, axis=-1)))\n",
    "    return tf.gather(adjusted_boxes, idx[:, 0]), tf.gather(classes, idx[:, 0])\n",
    "\n",
    "def _random_crop(image, boxes, classes):\n",
    "    min_obj_covered = 0.1\n",
    "    area_range = [0.05, 1]\n",
    "    aspect_ratio_range = [0.667, 1.334]\n",
    "\n",
    "    boxes = absolute_to_relative(boxes, tf.shape(image))\n",
    "\n",
    "    start, size, crop_box = tf.python.image.sample_distorted_bounding_box_v2(\n",
    "        image_size=tf.shape(image),\n",
    "        bounding_boxes=tf.expand_dims(swap_xy(boxes), axis=0),\n",
    "        min_object_covered=min_obj_covered,\n",
    "        area_range=area_range,\n",
    "        aspect_ratio_range=aspect_ratio_range)\n",
    "    \n",
    "    crop_box = relative_to_absolute(swap_xy(crop_box[0]), tf.shape(image))\n",
    "\n",
    "    cropped_image = tf.slice(image, start, size)\n",
    "    cropped_image.set_shape([None, None, 3])\n",
    "\n",
    "    boxes = relative_to_absolute(boxes, tf.shape(image))\n",
    "    adjusted_boxes, adjusted_classes = filter_and_adjust_labels(crop_box, boxes, classes)\n",
    "\n",
    "    return cropped_image, adjusted_boxes, adjusted_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image, boxes, cls_ids in dataset.take(20):\n",
    "\n",
    "    img, bx, cx = _random_crop(image, boxes, cls_ids)\n",
    "    categories = [['circle', 'rectangle'][_id] for _id in cls_ids]\n",
    "    cats = [['circle', 'rectangle'][_id] for _id in cx]\n",
    "\n",
    "    imshow_multiple([draw_boxes_cv2(image, boxes, categories), draw_boxes_cv2(img, bx, cats)], ['original', 'cropped'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "include_colab_link": true,
   "name": "colab_train.ipynb",
   "provenance": []
  },
  "environment": {
   "name": "tf2-cpu.2-1.m47",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-cpu.2-1:m47"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
